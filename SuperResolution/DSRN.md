## Image Super-Resolution via Dual-State Recurrent Networks

### 摘要

​		图像超分辨率（SR）的进步最近得益于深度神经网络的快速发展。受到这些最新发现的启发，我们注意到许多最新的深度SR架构可以重新构造为具有有限展开的单状态循环神经网络（RNN）。在本文中，我们基于此紧凑的RNN视图探索SR的新结构，从而使我们进了双状态设计，即双状态循环网络（Dual-State Recurrent Network：DSRN）。与它的在固定空间分辨率上操作的单状态副本相比，DSRN联合利用低分辨率（low resolution：LR）和高分辨率（high resolution：HR）信号。循环信号通过延迟反馈在两个方向（LR至HR和HR至LR）之间在这些状态之间交换。对基准数据集和近期挑战的大量定量和定性评估表明，就内存消耗和预测准确性而言，所提出的DSRN相对于最新算法表现良好。

### 1. 引言

​		在单图像超分辨率（SR）问题中，目标是从单幅低分辨率图像恢复高分辨率图像。最近几年，由于深度神经网络的快速发展，SR性能明显得到提高。具体而言，CNN和残差学习[16]被广泛用于最近的SR工作[10、12、20、21、22、25、35、38]。

​		在这些方法中，持续观察到两个原则。第一是增加CNN模型的深度可以提高SR的性能；更多参数的更深模型可以表示LR到HR图像的更复杂映射。此外，增加网络深度扩大感受野大小，提高更多可以利用的上下文信息来重建丢失的HR分量。第二个原则是，添加残差连接（全局[20]、局部[21]或二者的联合[35]）阻止梯度消失和爆炸，促进深度模型的训练。

​		虽然这些最近的模型已表现出有前途的性能，但是也有缺点。一个主要的问题是，通过添加新的层增加模型的深度引入更多参数，因此引起模型过拟合。同时，更大的模型需要更多的存储空间，这是在有限资源环境（例如移动系统）中部署的一个障碍。为了解决这个问题，受深度递归卷积网络（Deeply-Recursive Convolutional Network：DRCN）[21]启发的深度递归残差网络（Deep Recursive Residual Network：DRRN）[35]在不同的残差单元之间共享权重，并通过少量参数实现了最新的性能。

​		神经体系结构设计中的单独工作[6，24，39]最近显示，可以使用递归神经网络（RNN）更加紧凑地表示常用的深层结构。具体而言，Liao和Poggio[24]证明权重共享的ResNet等价于浅层的RNN。受它们的发现的启发，我们首先探索已有SR算法的神经架构和它们紧凑的RNN形式之间的连接。我们注意到先前具有递归计算和权重共享的SR模型（包括DRRN和DRCN）在单个空间分辨率（首先将bicubic插值用于LR图像得到期望的空间分辨率）上工作。这保证它们的模型结构可以表示为统一的单状态RNN。因此，DRRN和DRCN都可以看作是相同RNN结构在时间上的有限展开，但具有不同的过渡函数。这如图1所示，